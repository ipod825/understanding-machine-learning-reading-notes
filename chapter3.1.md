
# PAC Learning

In the last chapter, we have shown that for a finite hypothesis class $$ \mathcal{H}$$, as long as we have large enough sample ($$m \ge log(| \mathcal{H}|/\delta)/\epsilon$$), then applying ERM will output a hypothesis that is probably (at least $$1-\delta$$) approximately correct (general loss $$\le \epsilon$$). More generally, we define *Probably Approximately Correct* (PAC) learning as:

**PAC learnability**
A hypothesis class $$ \mathcal{H}$$ is PAC learnable if there exists a function $$m_{ \mathcal{H} }: (0,1)^2 \rightarrow \mathbb{N}$$ and a learning algorithm with the following property: For every $$\epsilon, \delta \in (0,1)$$, for every distribution $$ \mathcal{D}$$ over $$ \chi$$, and for every labeling function $$f: \chi \rightarrow \{0,1\}$$, if the realizable assumption holds with respect to $$ \mathcal{H}, \mathcal{D}, f$$, then running the algorithm on $$ m \ge m_{ \mathcal{H} }(\epsilon, \delta)$$ i.i.d examples generated by $$ \mathcal{D}$$ and labeled by $$f$$ will get a hypothesis such that, with probability of at least $$1-\delta$$ (over the choice of all possible samples of size $$m$$), the general loss $$L_{( \mathcal{D}, f )}(h)\le epsilon$$

In short, if increasing sample size guarantee you to have small general loss, then the hypothesis you choose is PAC learnable. Quantitatively, the number of sample size needed is decided by two parameters. The accuracy parameter $$\epsilon$$  how far the
output classifier can be from the optimal one (this corresponds to the “approximately correct”). The confidence parameter $$\delta$$ indicates how likely the classifier is to meet that accuracy requirement (corresponds to the “probably” part of “PAC”) 

Note that it is inevitable to introduce the confidence parameter. Because we use finite sample and there is always a small chance that the sample data is not representative for the underlying distribution $$ \mathcal{D}$$. 


